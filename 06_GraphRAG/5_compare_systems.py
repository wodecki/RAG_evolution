#!/usr/bin/env python3
"""
GraphRAG vs Naive RAG Comparison System
======================================

Compares GraphRAG and Naive RAG performance using GPT-5 ground truth
to demonstrate the advantages of graph-based retrieval for structured queries.
"""

from dotenv import load_dotenv
load_dotenv(override=True)

import json
import time
import asyncio
import importlib.util
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SystemComparator:
    """Compares GraphRAG, Naive RAG, and Ground Truth systems."""

    def __init__(self):
        """Initialize the comparator."""
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)

        # Systems will be initialized when needed
        self.graph_rag_system = None
        self.naive_rag_system = None

        logger.info("✓ System Comparator initialized")

    def load_ground_truth(self) -> Dict[str, Any]:
        """Load ground truth answers generated by GPT-5."""
        ground_truth_file = self.results_dir / "ground_truth_answers.json"

        if not ground_truth_file.exists():
            logger.info(f"Ground truth file not found: {ground_truth_file}")
            logger.info("Generating ground truth automatically...")

            import subprocess
            result = subprocess.run(
                ["uv", "run", "python", "utils/generate_ground_truth.py"],
                capture_output=True,
                text=True
            )

            if result.returncode != 0:
                raise RuntimeError(
                    f"Failed to generate ground truth:\n"
                    f"STDOUT: {result.stdout}\n"
                    f"STDERR: {result.stderr}"
                )

            if not ground_truth_file.exists():
                raise FileNotFoundError(
                    f"Ground truth generation completed but file still not found: {ground_truth_file}"
                )

            logger.info("✓ Ground truth generated successfully")
        else:
            logger.info(f"✓ Using existing ground truth: {ground_truth_file}")

        with open(ground_truth_file, 'r') as f:
            data = json.load(f)

        logger.info(f"✓ Loaded ground truth with {len(data['ground_truth_answers'])} questions")
        return data

    def initialize_graph_rag_system(self):
        """Initialize the GraphRAG system from file 3."""
        try:
            # Import the GraphRAG system dynamically
            spec = importlib.util.spec_from_file_location(
                "graph_rag_module", "3_query_knowledge_graph.py"
            )
            graph_rag_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(graph_rag_module)

            self.graph_rag_system = graph_rag_module.CVGraphRAGSystem()
            logger.info("✓ GraphRAG system initialized")
            return True

        except Exception as e:
            logger.error(f"Failed to initialize GraphRAG system: {e}")
            return False

    def initialize_naive_rag_system(self):
        """Initialize the Naive RAG system from file 4."""
        try:
            # Import the Naive RAG system dynamically
            spec = importlib.util.spec_from_file_location(
                "naive_rag_module", "4_naive_rag_cv.py"
            )
            naive_rag_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(naive_rag_module)

            self.naive_rag_system = naive_rag_module.NaiveRAGSystem()

            # Initialize the system
            if self.naive_rag_system.initialize_system():
                logger.info("✓ Naive RAG system initialized")
                return True
            else:
                logger.error("Failed to initialize Naive RAG system")
                return False

        except Exception as e:
            logger.error(f"Failed to initialize Naive RAG system: {e}")
            return False

    def run_graph_rag_query(self, question: str) -> Dict[str, Any]:
        """Run a query through the GraphRAG system."""
        try:
            start_time = time.time()
            result = self.graph_rag_system.query_graph(question)
            execution_time = time.time() - start_time

            return {
                "question": question,
                "answer": result.get("answer", "No answer"),
                "cypher_query": result.get("cypher_query", ""),
                "execution_time": execution_time,
                "success": result.get("success", False),
                "system": "graphrag"
            }

        except Exception as e:
            logger.error(f"GraphRAG query failed for '{question}': {e}")
            return {
                "question": question,
                "answer": f"Error: {str(e)}",
                "cypher_query": "",
                "execution_time": 0,
                "success": False,
                "system": "graphrag",
                "error": str(e)
            }

    def run_naive_rag_query(self, question: str) -> Dict[str, Any]:
        """Run a query through the Naive RAG system."""
        try:
            result = self.naive_rag_system.query(question)

            return {
                "question": question,
                "answer": result.get("answer", "No answer"),
                "execution_time": result.get("execution_time", 0),
                "num_chunks_retrieved": result.get("num_chunks_retrieved", 0),
                "success": result.get("success", False),
                "system": "naive_rag"
            }

        except Exception as e:
            logger.error(f"Naive RAG query failed for '{question}': {e}")
            return {
                "question": question,
                "answer": f"Error: {str(e)}",
                "execution_time": 0,
                "num_chunks_retrieved": 0,
                "success": False,
                "system": "naive_rag",
                "error": str(e)
            }

    def evaluate_answer_quality(self, ground_truth: str, system_answer: str, question_type: str) -> Dict[str, Any]:
        """Evaluate how well a system answer matches the ground truth."""

        # Simple evaluation criteria
        evaluation = {
            "exact_match": ground_truth.strip().lower() == system_answer.strip().lower(),
            "contains_key_info": False,
            "numerical_accuracy": None,
            "completeness_score": 0.0,  # 0-1 scale
            "quality_score": 0.0  # 0-1 scale
        }

        # Extract key information based on question type
        if question_type == "counting":
            # Look for numbers in both answers
            import re
            gt_numbers = re.findall(r'\d+', ground_truth)
            sys_numbers = re.findall(r'\d+', system_answer)

            if gt_numbers and sys_numbers:
                try:
                    gt_num = int(gt_numbers[0])
                    sys_num = int(sys_numbers[0])
                    evaluation["numerical_accuracy"] = abs(gt_num - sys_num) == 0
                    evaluation["contains_key_info"] = True
                    evaluation["quality_score"] = 1.0 if gt_num == sys_num else 0.0
                except ValueError:
                    pass

        elif question_type in ["filtering", "listing"]:
            # Check if system answer contains key entities from ground truth
            gt_words = set(ground_truth.lower().split())
            sys_words = set(system_answer.lower().split())

            # Look for name-like words (capitalized in original)
            gt_names = set(word for word in ground_truth.split() if word and word[0].isupper())
            sys_names = set(word for word in system_answer.split() if word and word[0].isupper())

            if gt_names:
                name_overlap = len(gt_names.intersection(sys_names)) / len(gt_names)
                evaluation["completeness_score"] = name_overlap
                evaluation["quality_score"] = name_overlap
                evaluation["contains_key_info"] = name_overlap > 0

        elif question_type == "aggregation":
            # Similar to counting for numerical results
            import re
            gt_nums = re.findall(r'\d+\.?\d*', ground_truth)
            sys_nums = re.findall(r'\d+\.?\d*', system_answer)

            if gt_nums and sys_nums:
                try:
                    gt_val = float(gt_nums[0])
                    sys_val = float(sys_nums[0])
                    # Allow small differences for averages
                    diff = abs(gt_val - sys_val) / max(gt_val, 1)
                    evaluation["numerical_accuracy"] = diff < 0.1  # 10% tolerance
                    evaluation["quality_score"] = max(0, 1.0 - diff)
                    evaluation["contains_key_info"] = True
                except ValueError:
                    pass

        else:
            # General text comparison
            gt_words = set(ground_truth.lower().split())
            sys_words = set(system_answer.lower().split())

            if gt_words:
                word_overlap = len(gt_words.intersection(sys_words)) / len(gt_words)
                evaluation["completeness_score"] = word_overlap
                evaluation["quality_score"] = word_overlap
                evaluation["contains_key_info"] = word_overlap > 0.3

        return evaluation

    async def run_full_comparison(self) -> Dict[str, Any]:
        """Run complete comparison between all systems."""
        # Load ground truth
        ground_truth_data = self.load_ground_truth()
        questions = ground_truth_data["ground_truth_answers"]

        # Initialize systems
        if not self.initialize_graph_rag_system():
            raise Exception("Failed to initialize GraphRAG system")

        if not self.initialize_naive_rag_system():
            raise Exception("Failed to initialize Naive RAG system")

        logger.info(f"Starting comparison with {len(questions)} questions...")

        # Run all comparisons
        results = []
        for i, ground_truth_item in enumerate(questions):
            question = ground_truth_item["question"]
            category = ground_truth_item["category"]
            ground_truth_answer = ground_truth_item["ground_truth_answer"]

            logger.info(f"\n[{i+1}/{len(questions)}] Processing: {question[:50]}...")

            # Run GraphRAG query
            logger.info("  Running GraphRAG...")
            graph_result = self.run_graph_rag_query(question)

            # Run Naive RAG query
            logger.info("  Running Naive RAG...")
            naive_result = self.run_naive_rag_query(question)

            # Evaluate both answers
            graph_evaluation = self.evaluate_answer_quality(
                ground_truth_answer, graph_result["answer"], category
            )
            naive_evaluation = self.evaluate_answer_quality(
                ground_truth_answer, naive_result["answer"], category
            )

            # Compile comparison
            comparison = {
                "question_index": i + 1,
                "question": question,
                "category": category,
                "ground_truth": ground_truth_answer,

                "graphrag": {
                    "answer": graph_result["answer"],
                    "cypher_query": graph_result.get("cypher_query", ""),
                    "execution_time": graph_result["execution_time"],
                    "success": graph_result["success"],
                    "evaluation": graph_evaluation
                },

                "naive_rag": {
                    "answer": naive_result["answer"],
                    "chunks_retrieved": naive_result.get("num_chunks_retrieved", 0),
                    "execution_time": naive_result["execution_time"],
                    "success": naive_result["success"],
                    "evaluation": naive_evaluation
                }
            }

            results.append(comparison)

            # Small delay to be nice to APIs
            await asyncio.sleep(0.5)

        # Compile final comparison data
        comparison_data = {
            "metadata": {
                "comparison_date": datetime.now().isoformat(),
                "total_questions": len(results),
                "ground_truth_source": "GPT-5",
                "systems_compared": ["GraphRAG", "Naive RAG"]
            },
            "results": results,
            "summary": self.generate_summary(results)
        }

        return comparison_data

    def generate_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate summary statistics for the comparison."""
        graph_wins = 0
        naive_wins = 0
        ties = 0

        graph_quality_scores = []
        naive_quality_scores = []

        category_stats = {}

        for result in results:
            category = result["category"]
            graph_score = result["graphrag"]["evaluation"]["quality_score"]
            naive_score = result["naive_rag"]["evaluation"]["quality_score"]

            graph_quality_scores.append(graph_score)
            naive_quality_scores.append(naive_score)

            # Track category performance
            if category not in category_stats:
                category_stats[category] = {
                    "total": 0,
                    "graph_wins": 0,
                    "naive_wins": 0,
                    "ties": 0
                }

            category_stats[category]["total"] += 1

            # Determine winner
            if graph_score > naive_score:
                graph_wins += 1
                category_stats[category]["graph_wins"] += 1
            elif naive_score > graph_score:
                naive_wins += 1
                category_stats[category]["naive_wins"] += 1
            else:
                ties += 1
                category_stats[category]["ties"] += 1

        import statistics

        summary = {
            "overall_performance": {
                "graphrag_wins": graph_wins,
                "naive_rag_wins": naive_wins,
                "ties": ties,
                "graphrag_win_rate": graph_wins / len(results),
                "naive_rag_win_rate": naive_wins / len(results)
            },
            "quality_scores": {
                "graphrag_avg": statistics.mean(graph_quality_scores),
                "naive_rag_avg": statistics.mean(naive_quality_scores),
                "graphrag_median": statistics.median(graph_quality_scores),
                "naive_rag_median": statistics.median(naive_quality_scores)
            },
            "category_breakdown": category_stats
        }

        return summary

    def save_comparison_results(self, comparison_data: Dict[str, Any]) -> Path:
        """Save comparison results to file."""
        output_file = self.results_dir / "system_comparison_results.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, indent=2, ensure_ascii=False)

        logger.info(f"✓ Comparison results saved to: {output_file}")
        return output_file

    def generate_comparison_table(self, comparison_data: Dict[str, Any]) -> str:
        """Generate a readable comparison table."""
        results = comparison_data["results"]
        summary = comparison_data["summary"]

        table_lines = []
        table_lines.append("# GraphRAG vs Naive RAG Comparison Results")
        table_lines.append("")
        table_lines.append("## Summary")
        table_lines.append(f"- **GraphRAG Wins**: {summary['overall_performance']['graphrag_wins']}")
        table_lines.append(f"- **Naive RAG Wins**: {summary['overall_performance']['naive_rag_wins']}")
        table_lines.append(f"- **Ties**: {summary['overall_performance']['ties']}")
        table_lines.append(f"- **GraphRAG Win Rate**: {summary['overall_performance']['graphrag_win_rate']:.1%}")
        table_lines.append("")
        table_lines.append(f"- **GraphRAG Avg Quality**: {summary['quality_scores']['graphrag_avg']:.2f}")
        table_lines.append(f"- **Naive RAG Avg Quality**: {summary['quality_scores']['naive_rag_avg']:.2f}")
        table_lines.append("")

        # Category breakdown
        table_lines.append("## Performance by Category")
        table_lines.append("")
        for category, stats in summary["category_breakdown"].items():
            win_rate = stats["graph_wins"] / stats["total"]
            table_lines.append(f"### {category.title()}")
            table_lines.append(f"- GraphRAG: {stats['graph_wins']}/{stats['total']} ({win_rate:.1%})")
            table_lines.append("")

        # Detailed results table
        table_lines.append("## Detailed Results")
        table_lines.append("")
        table_lines.append("| # | Question | Category | GraphRAG Answer | Naive RAG Answer | Ground Truth | Winner |")
        table_lines.append("|---|----------|----------|-----------------|------------------|--------------|--------|")

        for result in results:
            idx = result["question_index"]
            question = result["question"][:50] + "..." if len(result["question"]) > 50 else result["question"]
            category = result["category"]

            graph_answer = result["graphrag"]["answer"][:30] + "..." if len(result["graphrag"]["answer"]) > 30 else result["graphrag"]["answer"]
            naive_answer = result["naive_rag"]["answer"][:30] + "..." if len(result["naive_rag"]["answer"]) > 30 else result["naive_rag"]["answer"]
            ground_truth = result["ground_truth"][:30] + "..." if len(result["ground_truth"]) > 30 else result["ground_truth"]

            graph_score = result["graphrag"]["evaluation"]["quality_score"]
            naive_score = result["naive_rag"]["evaluation"]["quality_score"]

            if graph_score > naive_score:
                winner = "GraphRAG ✅"
            elif naive_score > graph_score:
                winner = "Naive RAG ✅"
            else:
                winner = "Tie ⚖️"

            # Escape pipes in text
            question = question.replace("|", "\\|")
            graph_answer = graph_answer.replace("|", "\\|")
            naive_answer = naive_answer.replace("|", "\\|")
            ground_truth = ground_truth.replace("|", "\\|")

            table_lines.append(f"| {idx} | {question} | {category} | {graph_answer} | {naive_answer} | {ground_truth} | {winner} |")

        return "\n".join(table_lines)

    def display_results(self, comparison_data: Dict[str, Any]) -> None:
        """Display comparison results."""
        summary = comparison_data["summary"]

        print("\n" + "="*80)
        print("GRAPHRAG vs NAIVE RAG COMPARISON RESULTS")
        print("="*80)

        print(f"\n📊 OVERALL PERFORMANCE:")
        print(f"GraphRAG Wins: {summary['overall_performance']['graphrag_wins']}")
        print(f"Naive RAG Wins: {summary['overall_performance']['naive_rag_wins']}")
        print(f"Ties: {summary['overall_performance']['ties']}")
        print(f"GraphRAG Win Rate: {summary['overall_performance']['graphrag_win_rate']:.1%}")

        print(f"\n🎯 QUALITY SCORES:")
        print(f"GraphRAG Average: {summary['quality_scores']['graphrag_avg']:.2f}")
        print(f"Naive RAG Average: {summary['quality_scores']['naive_rag_avg']:.2f}")

        print(f"\n📋 PERFORMANCE BY CATEGORY:")
        for category, stats in summary["category_breakdown"].items():
            win_rate = stats["graph_wins"] / stats["total"]
            print(f"{category.title()}: GraphRAG {stats['graph_wins']}/{stats['total']} ({win_rate:.1%})")


async def main():
    """Main comparison function with enhanced output."""
    print("🚀 GraphRAG vs Naive RAG Complete Comparison Workflow")
    print("=" * 60)

    try:
        # Check if CV data exists
        data_dir = Path("data/programmers")
        if not data_dir.exists() or not list(data_dir.glob("*.pdf")):
            print(f"\n⚠️  No CV PDFs found in {data_dir}")
            print("Please run: uv run python 1_generate_data.py")
            print("Then run: uv run python 2_data_to_knowledge_graph.py")
            return False

        print("\n✅ CV data found successfully!")

        # Step 1: Ground Truth (handled automatically by SystemComparator)
        print("\n" + "="*60)
        print("STEP 1: Load/Generate Ground Truth using GPT-5")
        print("="*60)

        comparator = SystemComparator()

        # Step 2: System Initialization and Comparison
        print("\n" + "="*60)
        print("STEP 2: Run Complete System Comparison")
        print("="*60)

        # Run full comparison
        print("\nStarting comprehensive comparison...")
        comparison_data = await comparator.run_full_comparison()

        # Save results
        output_file = comparator.save_comparison_results(comparison_data)

        # Generate markdown table
        markdown_table = comparator.generate_comparison_table(comparison_data)
        table_file = Path("results") / "comparison_table.md"
        with open(table_file, 'w') as f:
            f.write(markdown_table)

        # Display results
        comparator.display_results(comparison_data)

        # Final results summary
        print("\n" + "="*60)
        print("🎉 COMPARISON WORKFLOW COMPLETED SUCCESSFULLY!")
        print("="*60)

        print("\n📁 Generated Files:")
        results_dir = Path("results")
        if results_dir.exists():
            for file in sorted(results_dir.glob("*")):
                print(f"  • {file}")

        print("\n📊 Key Results:")
        print(f"  • Ground Truth: {results_dir}/ground_truth_answers.json")
        print(f"  • Comparison Data: {output_file}")
        print(f"  • Comparison Table: {table_file}")

        print("\n🔗 Next Steps:")
        print(f"  1. Review the comparison table: cat {table_file}")
        print(f"  2. Analyze detailed results: cat {output_file}")
        print("  3. Browse Neo4j graph: http://localhost:7474 (neo4j/password123)")

        return True

    except Exception as e:
        logger.error(f"Comparison failed: {e}")
        print(f"\n💥 Workflow failed: {e}")
        print("Check the errors above for details.")
        return False


if __name__ == "__main__":
    asyncio.run(main())