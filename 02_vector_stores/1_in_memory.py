"""
InMemoryVectorStore Demo
========================

Demonstrates the simplest vector store option - storing embeddings in memory.
Perfect for development, testing, and small datasets where persistence isn't needed.
"""

from langchain_community.document_loaders import DirectoryLoader
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.text_splitter import RecursiveCharacterTextSplitter

from dotenv import load_dotenv
load_dotenv(override=True)

# Load and chunk documents
loader = DirectoryLoader("data/scientists_bios")
docs = loader.load()
print(f"Loaded {len(docs)} documents")

# Split documents into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(docs)
print(f"Created {len(chunks)} chunks")

# Create embeddings and in-memory vector store
embeddings = OpenAIEmbeddings()
in_memory_store = InMemoryVectorStore(embeddings)

# Add documents to store
in_memory_store.add_documents(documents=chunks)
print(f"‚úÖ Added {len(chunks)} chunks to in-memory store")

# Create retriever
retriever = in_memory_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

# Test similarity search directly
test_query = "What did Ada Lovelace contribute to computing?"
similar_docs = in_memory_store.similarity_search(test_query, k=3)

print(f"Query: {test_query}")
print(f"Found {len(similar_docs)} similar chunks:")
for i, doc in enumerate(similar_docs, 1):
    print(f"\nChunk {i}: {doc.page_content[:200]}...")

# Create RAG chain
print("\nüîó Creating RAG chain...")
llm = ChatOpenAI(model="gpt-4o-mini")

prompt = ChatPromptTemplate.from_template("""
You are an assistant for question-answering tasks.
Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know.
Use three sentences maximum and keep the answer concise.

Question: {question}

Context: {context}

Answer:
""")

in_memory_rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Demo questions
questions = [
    "What programming concept did Ada Lovelace pioneer?",
    "How did Einstein's work change physics?",
    "What was Marie Curie's most important discovery?"
]

print("\n" + "="*50)
print("IN-MEMORY VECTOR STORE RAG DEMO")
print("="*50)

for i, question in enumerate(questions, 1):
    print(f"\nQ{i}: {question}")
    print("-" * 40)
    response = in_memory_rag_chain.invoke(question)
    print(f"A{i}: {response}")

# Demonstrate vector store properties
print("\n" + "="*50)
print("IN-MEMORY STORE PROPERTIES")
print("="*50)
print("‚úÖ Advantages:")
print("  ‚Ä¢ Fast - no disk I/O overhead")
print("  ‚Ä¢ Simple - no external dependencies")
print("  ‚Ä¢ Perfect for development/testing")
print("  ‚Ä¢ No setup required")

print("\n‚ùå Limitations:")
print("  ‚Ä¢ Data lost when process ends")
print("  ‚Ä¢ Limited by available RAM")
print("  ‚Ä¢ No data sharing between processes")
print("  ‚Ä¢ Rebuild required on restart")

print(f"\nüí° Current store contains {len(chunks)} chunks in memory")
print("üí° Try: in_memory_rag_chain.invoke('Your question here')")
print("üí° Or: in_memory_store.similarity_search('Your query', k=5)")